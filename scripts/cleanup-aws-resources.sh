#!/bin/bash

# AWS Resource Cleanup Script
# This script provides a comprehensive cleanup of all AWS resources created by the GitHub Actions
# Use this as a backup method if the destroy workflow fails

set -e

# Configuration
AWS_REGION="us-east-1"
CLUSTER_NAME="thrive-cluster-test"
PROJECT_TAG="Thrive_Cluster_Test"

echo "ğŸ§¹ AWS Resource Cleanup Script"
echo "================================"
echo "âš ï¸  WARNING: This will delete ALL resources associated with this project!"
echo ""

# Confirmation
read -p "Are you sure you want to proceed? Type 'DELETE' to confirm: " confirmation
if [ "$confirmation" != "DELETE" ]; then
    echo "âŒ Cleanup cancelled"
    exit 1
fi

echo "âœ… Confirmation received. Starting cleanup..."

# Function to check if resource exists
check_resource() {
    local resource_type="$1"
    local resource_name="$2"
    local command="$3"
    
    if eval "$command" >/dev/null 2>&1; then
        echo "ğŸ“¦ Found $resource_type: $resource_name"
        return 0
    else
        echo "â„¹ï¸  $resource_type not found: $resource_name"
        return 1
    fi
}

# Function to delete resource safely
delete_resource() {
    local resource_type="$1"
    local resource_name="$2"
    local delete_command="$3"
    
    echo "ğŸ—‘ï¸  Deleting $resource_type: $resource_name"
    if eval "$delete_command" 2>/dev/null; then
        echo "âœ… Successfully deleted $resource_type: $resource_name"
    else
        echo "âš ï¸  Failed to delete $resource_type: $resource_name (may already be deleted)"
    fi
}

echo ""
echo "ğŸ” Step 1: Checking and cleaning up Kubernetes resources..."

# Check if EKS cluster exists
if check_resource "EKS cluster" "$CLUSTER_NAME" "aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION"; then
    echo "ğŸ“¦ Found EKS cluster, cleaning up Kubernetes resources..."
    
    # Update kubeconfig
    aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME 2>/dev/null || echo "âš ï¸ Could not update kubeconfig"
    
    # Delete Kubernetes resources
    kubectl delete deployment hello-world --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete deployment"
    kubectl delete service hello-world-service --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete service"
    kubectl delete service hello-world-clusterip --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete clusterip service"
    kubectl delete ingress hello-world-ingress --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete ingress"
    kubectl delete hpa hello-world-hpa --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete HPA"
    kubectl delete networkpolicy hello-world-network-policy --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete network policy"
    kubectl delete podsecuritypolicy hello-world-psp --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not delete PSP"
    
    # Wait for resources to be deleted
    echo "â³ Waiting for Kubernetes resources to be deleted..."
    sleep 30
    
    # Force delete any remaining pods
    kubectl delete pods --all --force --grace-period=0 --ignore-not-found=true 2>/dev/null || echo "âš ï¸ Could not force delete pods"
    
    echo "âœ… Kubernetes resources cleaned up"
fi

echo ""
echo "ğŸ” Step 2: Cleaning up ECR resources..."

# Clean up ECR images
if check_resource "ECR repository" "hello-world" "aws ecr describe-repositories --repository-names hello-world --region $AWS_REGION"; then
    echo "ğŸ“¦ Found ECR repository, cleaning up images..."
    
    # List and delete all images
    IMAGES=$(aws ecr list-images --repository-name hello-world --region $AWS_REGION --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')
    if [ "$IMAGES" != "[]" ] && [ "$IMAGES" != "null" ]; then
        echo "ğŸ—‘ï¸ Deleting ECR images..."
        aws ecr batch-delete-image --repository-name hello-world --region $AWS_REGION --image-ids "$IMAGES" 2>/dev/null || echo "âš ï¸ Some images may have already been deleted"
    else
        echo "â„¹ï¸ No images found in ECR repository"
    fi
fi

echo ""
echo "ğŸ” Step 3: Cleaning up CloudWatch resources..."

# Clean up CloudWatch Log Groups
delete_resource "CloudWatch log group" "/aws/eks/$CLUSTER_NAME/cluster" "aws logs delete-log-group --log-group-name '/aws/eks/$CLUSTER_NAME/cluster' --region $AWS_REGION"

# Clean up CloudWatch Dashboards
DASHBOARDS=$(aws cloudwatch list-dashboards --region $AWS_REGION --query "DashboardEntries[?contains(DashboardName, '$CLUSTER_NAME')].DashboardName" --output text 2>/dev/null || echo "")
if [ -n "$DASHBOARDS" ]; then
    for dashboard in $DASHBOARDS; do
        delete_resource "CloudWatch dashboard" "$dashboard" "aws cloudwatch delete-dashboards --dashboard-names '$dashboard' --region $AWS_REGION"
    done
fi

# Clean up CloudWatch Alarms
ALARMS=$(aws cloudwatch describe-alarms --region $AWS_REGION --query "MetricAlarms[?contains(AlarmName, '$CLUSTER_NAME')].AlarmName" --output text 2>/dev/null || echo "")
if [ -n "$ALARMS" ]; then
    for alarm in $ALARMS; do
        delete_resource "CloudWatch alarm" "$alarm" "aws cloudwatch delete-alarms --alarm-names '$alarm' --region $AWS_REGION"
    done
fi

echo ""
echo "ğŸ” Step 4: Cleaning up SNS resources..."

# Clean up SNS Topic Subscriptions
TOPICS=$(aws sns list-topics --region $AWS_REGION --query "Topics[?contains(TopicArn, '$CLUSTER_NAME')].TopicArn" --output text 2>/dev/null || echo "")
if [ -n "$TOPICS" ]; then
    for topic in $TOPICS; do
        echo "ğŸ“§ Found SNS topic: $topic"
        # List subscriptions
        SUBSCRIPTIONS=$(aws sns list-subscriptions-by-topic --topic-arn "$topic" --region $AWS_REGION --query 'Subscriptions[?SubscriptionArn != `PendingConfirmation`].SubscriptionArn' --output text 2>/dev/null || echo "")
        if [ -n "$SUBSCRIPTIONS" ]; then
            for sub in $SUBSCRIPTIONS; do
                delete_resource "SNS subscription" "$sub" "aws sns unsubscribe --subscription-arn '$sub' --region $AWS_REGION"
            done
        fi
    done
fi

echo ""
echo "ğŸ” Step 5: Running Terraform destroy..."

# Run terraform destroy
if [ -f "terraform.tfstate" ] && [ -s "terraform.tfstate" ]; then
    echo "ğŸ“¦ Found Terraform state, running destroy..."
    echo "ğŸ“‹ Resources in state:"
    terraform state list 2>/dev/null || echo "â„¹ï¸ No resources in state"
    echo ""
    terraform destroy -auto-approve
    echo "âœ… Terraform destroy completed"
else
    echo "â„¹ï¸ No Terraform state found, skipping terraform destroy"
    echo "ğŸ’¡ Note: If you're running this manually, make sure you have the correct terraform.tfstate file"
    echo "ğŸ’¡ The state file should be from the same deployment you want to destroy"
fi

echo ""
echo "ğŸ” Step 6: Final verification..."

# Final verification
echo "ğŸ” Verifying cleanup..."

# Check EKS cluster
if aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION 2>/dev/null; then
    echo "âš ï¸ EKS cluster still exists"
else
    echo "âœ… EKS cluster destroyed"
fi

# Check ECR repository
if aws ecr describe-repositories --repository-names hello-world --region $AWS_REGION 2>/dev/null; then
    echo "âš ï¸ ECR repository still exists"
else
    echo "âœ… ECR repository destroyed"
fi

# Check VPC
VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=$PROJECT_TAG" --query 'Vpcs[0].VpcId' --output text --region $AWS_REGION 2>/dev/null || echo "None")
if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ]; then
    echo "âš ï¸ VPC still exists: $VPC_ID"
else
    echo "âœ… VPC destroyed"
fi

# Check IAM Roles
for role in "thrive-cluster-test-cluster-role" "thrive-cluster-test-node-role" "thrive-cluster-test-github-actions-role" "hello-world-pod-role"; do
    if aws iam get-role --role-name "$role" --region $AWS_REGION 2>/dev/null; then
        echo "âš ï¸ IAM role still exists: $role"
    else
        echo "âœ… IAM role destroyed: $role"
    fi
done

# Check Secrets Manager
for secret in "thrive-cluster-test-github-actions-credentials" "thrive-cluster-test-app-secrets"; do
    if aws secretsmanager describe-secret --secret-id "$secret" --region $AWS_REGION 2>/dev/null; then
        echo "âš ï¸ Secret still exists: $secret"
    else
        echo "âœ… Secret destroyed: $secret"
    fi
done

echo ""
echo "ğŸ¯ Cleanup Complete!"
echo "==================="
echo ""
echo "ğŸ“‹ Resources that were cleaned up:"
echo "  âœ… EKS Cluster and Node Groups"
echo "  âœ… VPC, Subnets, Internet Gateway, Route Tables"
echo "  âœ… ECR Repository and Images"
echo "  âœ… IAM Roles and Policies (EKS, GitHub Actions, Pod)"
echo "  âœ… Secrets Manager Secrets"
echo "  âœ… CloudWatch Log Groups, Dashboards, and Alarms"
echo "  âœ… SNS Topics and Subscriptions"
echo "  âœ… AWS Budgets and Billing Alarms"
echo "  âœ… OIDC Identity Providers"
echo "  âœ… Kubernetes Resources (Deployments, Services, Ingress, etc.)"
echo ""
echo "ğŸ’° All AWS resources have been destroyed"
echo "ğŸ’¡ You can now run the deploy workflow to recreate everything"
echo "ğŸ“Š Check your AWS Console to verify all resources are gone"
echo "ğŸ”’ Your AWS account is now clean and ready for the next deployment"
