name: Destroy Infrastructure

on:
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type "DESTROY" to confirm infrastructure destruction'
        required: true
        default: ''
        type: string

env:
  AWS_REGION: us-east-1

jobs:
  destroy-infrastructure:
    runs-on: ubuntu-latest
    environment: AWS_OIDC
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Confirm destruction
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "‚ùå Destruction not confirmed. Please type 'DESTROY' to proceed."
            exit 1
          fi
          echo "‚úÖ Destruction confirmed. Proceeding with infrastructure teardown..."

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/thrive-cluster-test-github-actions-role
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions

      - name: Set GitHub repo environment variable
        run: echo "GITHUB_REPO=${{ github.repository }}" >> $GITHUB_ENV

      - name: Get AWS Account ID
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
          terraform_wrapper: false

      - name: Verify Backend Infrastructure
        run: |
          echo "üîç Verifying Terraform backend infrastructure..."
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket "thrive-cluster-test-terraform-state" 2>/dev/null; then
            echo "‚úÖ S3 backend bucket exists"
          else
            echo "‚ùå S3 backend bucket not found!"
            echo "üí° Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi
          
          # Check if DynamoDB table exists
          if aws dynamodb describe-table --table-name "thrive-cluster-test-terraform-locks" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "‚úÖ DynamoDB lock table exists"
          else
            echo "‚ùå DynamoDB lock table not found!"
            echo "üí° Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi

      - name: Terraform Init
        run: |
          echo "üîÑ Initializing Terraform with S3 backend..."
          terraform init -backend-config="bucket=thrive-cluster-test-terraform-state" -backend-config="key=terraform.tfstate" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=thrive-cluster-test-terraform-locks" -backend-config="encrypt=true"
          echo "‚úÖ Terraform initialized with S3 backend"

      - name: Verify Terraform state
        run: |
          echo "üîç Verifying Terraform state from S3 backend..."
          echo "üìã Resources in state:"
          terraform state list 2>/dev/null || echo "‚ÑπÔ∏è No resources in state or state is empty"
          echo "‚úÖ State file is accessible from S3 backend"

      - name: Check if resources exist
        id: check-resources
        run: |
          if terraform state list 2>/dev/null | grep -q .; then
            echo "resources_exist=true" >> $GITHUB_OUTPUT
            echo "üìã Found existing resources to destroy"
            terraform state list
          else
            echo "resources_exist=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è No resources found in Terraform state"
          fi

      - name: Check and handle Terraform state lock
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üîç Checking for existing Terraform state locks..."
          
          # Try to run terraform plan to check for locks
          if ! terraform plan -var="github_repo=$GITHUB_REPO" -var="aws_account_id=$AWS_ACCOUNT_ID" -out=/dev/null 2>&1 | grep -q "Error acquiring the state lock"; then
            echo "‚úÖ No state lock detected"
          else
            echo "‚ö†Ô∏è State lock detected, attempting to force unlock..."
            
            # Extract lock ID from the error message
            LOCK_ID=$(terraform plan -var="github_repo=$GITHUB_REPO" -var="aws_account_id=$AWS_ACCOUNT_ID" -out=/dev/null 2>&1 | grep -o 'ID:\s*[a-f0-9-]*' | sed 's/ID:\s*//' | xargs || echo "")
            
            if [ -n "$LOCK_ID" ]; then
              echo "üîì Force unlocking state with ID: '$LOCK_ID'"
              echo "üîç Lock ID length: ${#LOCK_ID}"
              terraform force-unlock -force "$LOCK_ID" || echo "‚ö†Ô∏è Could not unlock state, will try destroy anyway"
            else
              echo "‚ö†Ô∏è Could not extract lock ID, will try destroy anyway"
            fi
          fi

      - name: Terraform Destroy
        if: steps.check-resources.outputs.resources_exist == 'true'
        timeout-minutes: 45
        run: |
          echo "üí• Destroying infrastructure with Terraform..."
          echo "‚è∞ This operation has a 45-minute timeout"
          echo "üìã Some resources (like VPC subnets) may take several minutes to destroy"
          
          # Try terraform destroy with lock handling
          if timeout 1800 terraform destroy \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_account_id=$AWS_ACCOUNT_ID" \
            -auto-approve; then
            echo "‚úÖ Infrastructure destroyed successfully!"
          else
            echo "‚ö†Ô∏è Terraform destroy failed, checking for state lock issues..."
            
            # Check if it's a lock issue and try to force unlock
            if terraform destroy \
              -var="github_repo=$GITHUB_REPO" \
              -var="aws_account_id=$AWS_ACCOUNT_ID" \
              -auto-approve 2>&1 | grep -q "Error acquiring the state lock"; then
              
              echo "üîì State lock detected, attempting force unlock and retry..."
              
              # Extract lock ID and force unlock
              LOCK_ID=$(terraform destroy \
                -var="github_repo=$GITHUB_REPO" \
                -var="aws_account_id=$AWS_ACCOUNT_ID" \
                -auto-approve 2>&1 | grep -o 'ID:\s*[a-f0-9-]*' | sed 's/ID:\s*//' | xargs || echo "")
              
              if [ -n "$LOCK_ID" ]; then
                echo "üîì Force unlocking state with ID: '$LOCK_ID'"
                echo "üîç Lock ID length: ${#LOCK_ID}"
                terraform force-unlock -force "$LOCK_ID"
                
                echo "üîÑ Retrying terraform destroy after unlock..."
                terraform destroy \
                  -var="github_repo=$GITHUB_REPO" \
                  -var="aws_account_id=$AWS_ACCOUNT_ID" \
                  -auto-approve
                echo "‚úÖ Infrastructure destroyed successfully after unlock!"
              else
                echo "‚ùå Could not extract lock ID, destroy failed"
                exit 1
              fi
            else
              echo "‚ùå Terraform destroy failed for reasons other than state lock"
              echo "üîç Checking if it's a timeout issue..."
              
              # Check if it's a timeout (exit code 124)
              if [ $? -eq 124 ]; then
                echo "‚è∞ Terraform destroy timed out after 30 minutes"
                echo "üîÑ Attempting to continue with manual cleanup of remaining resources..."
                echo "‚ÑπÔ∏è Some resources may still exist and need manual cleanup"
              else
                echo "‚ùå Terraform destroy failed with error code: $?"
                exit 1
              fi
            fi
          fi

      - name: Clean up remaining Kubernetes resources
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üßπ Cleaning up any remaining Kubernetes resources..."
          # Check if EKS cluster still exists
          if aws eks describe-cluster --name thrive-cluster-test --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üì¶ EKS cluster still exists, cleaning up Kubernetes resources..."
            
            # Update kubeconfig
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name thrive-cluster-test || echo "‚ö†Ô∏è Could not update kubeconfig"
            
            # Delete all deployments, services, and ingress
            kubectl delete deployment hello-world --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete deployment"
            kubectl delete service hello-world-service --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete service"
            kubectl delete service hello-world-clusterip --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete clusterip service"
            kubectl delete ingress hello-world-ingress --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete ingress"
            kubectl delete hpa hello-world-hpa --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete HPA"
            kubectl delete networkpolicy hello-world-network-policy --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete network policy"
            kubectl delete podsecuritypolicy hello-world-psp --ignore-not-found=true || echo "‚ö†Ô∏è Could not delete PSP"
            
            # Wait for resources to be deleted
            echo "‚è≥ Waiting for Kubernetes resources to be deleted..."
            sleep 30
            
            # Force delete any remaining pods
            kubectl delete pods --all --force --grace-period=0 --ignore-not-found=true || echo "‚ö†Ô∏è Could not force delete pods"
            
            echo "‚úÖ Kubernetes resources cleaned up"
          else
            echo "‚ÑπÔ∏è EKS cluster destroyed by Terraform"
          fi

      - name: Clean up remaining ECR images
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üßπ Cleaning up any remaining ECR images..."
          # Check if ECR repository still exists
          if aws ecr describe-repositories --repository-names hello-world --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üì¶ ECR repository still exists, cleaning up images..."
            # List and delete all images
            IMAGES=$(aws ecr list-images --repository-name hello-world --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')
            if [ "$IMAGES" != "[]" ] && [ "$IMAGES" != "null" ]; then
              echo "üóëÔ∏è Deleting ECR images..."
              aws ecr batch-delete-image --repository-name hello-world --region ${{ env.AWS_REGION }} --image-ids "$IMAGES" || echo "‚ö†Ô∏è Some images may have already been deleted"
            else
              echo "‚ÑπÔ∏è No images found in ECR repository"
            fi
          else
            echo "‚ÑπÔ∏è ECR repository destroyed by Terraform"
          fi

      - name: Clean up remaining CloudWatch Log Groups
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üßπ Cleaning up any remaining CloudWatch Log Groups..."
          # Delete EKS cluster log group
          aws logs delete-log-group --log-group-name "/aws/eks/thrive-cluster-test/cluster" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è Log group not found or already deleted"
          echo "‚úÖ CloudWatch Log Groups cleaned up"

      - name: Clean up remaining SNS Topic Subscriptions
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üßπ Cleaning up any remaining SNS Topic Subscriptions..."
          # List and delete SNS topic subscriptions
          TOPICS=$(aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null || echo "")
          if [ -n "$TOPICS" ]; then
            for topic in $TOPICS; do
              echo "üìß Found SNS topic: $topic"
              # List subscriptions
              SUBSCRIPTIONS=$(aws sns list-subscriptions-by-topic --topic-arn "$topic" --region ${{ env.AWS_REGION }} --query 'Subscriptions[?SubscriptionArn != `PendingConfirmation`].SubscriptionArn' --output text 2>/dev/null || echo "")
              if [ -n "$SUBSCRIPTIONS" ]; then
                for sub in $SUBSCRIPTIONS; do
                  echo "üóëÔ∏è Deleting subscription: $sub"
                  aws sns unsubscribe --subscription-arn "$sub" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ö†Ô∏è Could not delete subscription"
                done
              fi
            done
          fi
          echo "‚úÖ SNS Topic Subscriptions cleaned up"

      - name: Force cleanup of stubborn resources
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üîß Force cleaning up any stubborn resources that Terraform couldn't destroy..."
          
          # Check for stubborn VPC resources
          echo "üîç Checking for stubborn VPC resources..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=Thrive_Cluster_Test" --query 'Vpcs[0].VpcId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ]; then
            echo "‚ö†Ô∏è VPC still exists: $VPC_ID"
            echo "üîß Attempting to force delete VPC resources..."
            
            # Delete any remaining ENIs
            aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query 'NetworkInterfaces[?Status==`available`].NetworkInterfaceId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-network-interface --network-interface-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è No ENIs to delete"
            
            # Delete any remaining security groups (except default)
            aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-security-group --group-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è No security groups to delete"
            
            # Delete any remaining subnets
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[].SubnetId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-subnet --subnet-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è No subnets to delete"
            
            # Delete any remaining route tables (except main)
            aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-route-table --route-table-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è No route tables to delete"
            
            # Delete internet gateway
            IGW_ID=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[0].InternetGatewayId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
            if [ "$IGW_ID" != "None" ] && [ "$IGW_ID" != "null" ]; then
              aws ec2 detach-internet-gateway --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è Could not detach IGW"
              aws ec2 delete-internet-gateway --internet-gateway-id "$IGW_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è Could not delete IGW"
            fi
            
            # Finally delete the VPC
            aws ec2 delete-vpc --vpc-id "$VPC_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è Could not delete VPC"
            echo "‚úÖ VPC cleanup attempted"
          else
            echo "‚ÑπÔ∏è No VPC found or already deleted"
          fi
          
          # Force delete any remaining ECR images
          aws ecr batch-delete-image --repository-name hello-world --region ${{ env.AWS_REGION }} --image-ids "$(aws ecr list-images --repository-name hello-world --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')" 2>/dev/null || echo "‚ÑπÔ∏è No ECR images to force delete"
          
          # Force delete any remaining CloudWatch log groups
          aws logs delete-log-group --log-group-name "/aws/eks/thrive-cluster-test/cluster" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è No log groups to force delete"
          
          # Force delete any remaining SNS subscriptions
          for topic in $(aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null || echo ""); do
            for sub in $(aws sns list-subscriptions-by-topic --topic-arn "$topic" --region ${{ env.AWS_REGION }} --query 'Subscriptions[?SubscriptionArn != `PendingConfirmation`].SubscriptionArn' --output text 2>/dev/null || echo ""); do
              aws sns unsubscribe --subscription-arn "$sub" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "‚ÑπÔ∏è Could not force delete subscription"
            done
          done
          
          echo "‚úÖ Force cleanup completed"

      - name: Verify destruction
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "üîç Verifying destruction of all resources..."
          
          # Check EKS cluster
          if aws eks describe-cluster --name thrive-cluster-test --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è EKS cluster still exists"
          else
            echo "‚úÖ EKS cluster destroyed"
          fi
          
          # Check ECR repository
          if aws ecr describe-repositories --repository-names hello-world --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è ECR repository still exists"
          else
            echo "‚úÖ ECR repository destroyed"
          fi
          
          # Check VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=Thrive_Cluster_Test" --query 'Vpcs[0].VpcId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ]; then
            echo "‚ö†Ô∏è VPC still exists: $VPC_ID"
          else
            echo "‚úÖ VPC destroyed"
          fi
          
          # Check IAM Roles
          if aws iam get-role --role-name thrive-cluster-test-cluster-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è EKS cluster IAM role still exists"
          else
            echo "‚úÖ EKS cluster IAM role destroyed"
          fi
          
          if aws iam get-role --role-name thrive-cluster-test-node-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è EKS node IAM role still exists"
          else
            echo "‚úÖ EKS node IAM role destroyed"
          fi
          
          if aws iam get-role --role-name thrive-cluster-test-github-actions-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è GitHub Actions IAM role still exists"
          else
            echo "‚úÖ GitHub Actions IAM role destroyed"
          fi
          
          if aws iam get-role --role-name hello-world-pod-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è Hello World pod IAM role still exists"
          else
            echo "‚úÖ Hello World pod IAM role destroyed"
          fi
          
          # Check Secrets Manager
          if aws secretsmanager describe-secret --secret-id thrive-cluster-test-github-actions-credentials --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è GitHub Actions secret still exists"
          else
            echo "‚úÖ GitHub Actions secret destroyed"
          fi
          
          if aws secretsmanager describe-secret --secret-id thrive-cluster-test-app-secrets --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è App secrets still exist"
          else
            echo "‚úÖ App secrets destroyed"
          fi
          
          # Check CloudWatch Log Groups
          if aws logs describe-log-groups --log-group-name-prefix "/aws/eks/thrive-cluster-test" --region ${{ env.AWS_REGION }} --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è CloudWatch log groups still exist"
          else
            echo "‚úÖ CloudWatch log groups destroyed"
          fi
          
          # Check CloudWatch Dashboards
          if aws cloudwatch list-dashboards --region ${{ env.AWS_REGION }} --query 'DashboardEntries[?contains(DashboardName, `thrive-cluster-test`)].DashboardName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è CloudWatch dashboards still exist"
          else
            echo "‚úÖ CloudWatch dashboards destroyed"
          fi
          
          # Check SNS Topics
          if aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è SNS topics still exist"
          else
            echo "‚úÖ SNS topics destroyed"
          fi
          
          # Check CloudWatch Alarms
          if aws cloudwatch describe-alarms --alarm-names thrive-cluster-test-billing-alarm --region ${{ env.AWS_REGION }} --query 'MetricAlarms[0].AlarmName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è CloudWatch alarms still exist"
          else
            echo "‚úÖ CloudWatch alarms destroyed"
          fi
          
          # Check Budgets
          if aws budgets describe-budgets --account-id $(aws sts get-caller-identity --query Account --output text) --region ${{ env.AWS_REGION }} --query 'Budgets[?contains(BudgetName, `thrive-cluster-test`)].BudgetName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è Budgets still exist"
          else
            echo "‚úÖ Budgets destroyed"
          fi
          
          # Check OIDC Providers
          if aws iam list-open-id-connect-providers --region ${{ env.AWS_REGION }} --query 'OpenIDConnectProviderList[?contains(Arn, `thrive-cluster-test`)].Arn' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "‚ö†Ô∏è OIDC providers still exist"
          else
            echo "‚úÖ OIDC providers destroyed"
          fi

      - name: Destruction Summary
        run: |
          echo "üéØ Infrastructure Destruction Complete!"
          echo ""
          echo "üìã Resources that were cleaned up:"
          echo "  ‚úÖ EKS Cluster and Node Groups"
          echo "  ‚úÖ VPC, Subnets, Internet Gateway, Route Tables"
          echo "  ‚úÖ ECR Repository and Images"
          echo "  ‚úÖ IAM Roles and Policies (EKS, GitHub Actions, Pod)"
          echo "  ‚úÖ Secrets Manager Secrets"
          echo "  ‚úÖ CloudWatch Log Groups, Dashboards, and Alarms"
          echo "  ‚úÖ SNS Topics and Subscriptions"
          echo "  ‚úÖ AWS Budgets and Billing Alarms"
          echo "  ‚úÖ OIDC Identity Providers"
          echo "  ‚úÖ Kubernetes Resources (Deployments, Services, Ingress, etc.)"
          echo ""
          echo "üí∞ All AWS resources have been destroyed"
          echo "üí° You can now run the deploy workflow to recreate everything"
          echo "üìä Check your AWS Console to verify all resources are gone"
          echo "üîí Your AWS account is now clean and ready for the next deployment"
