name: Destroy Infrastructure

on:
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type "DESTROY" to confirm infrastructure destruction'
        required: true
        default: ''
        type: string

env:
  AWS_REGION: us-east-1

jobs:
  destroy-infrastructure:
    runs-on: ubuntu-latest
    environment: AWS_OIDC
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Confirm destruction
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "âŒ Destruction not confirmed. Please type 'DESTROY' to proceed."
            exit 1
          fi
          echo "âœ… Destruction confirmed. Proceeding with infrastructure teardown..."

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/thrive-cluster-test-github-actions-role
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions

      - name: Set GitHub repo environment variable
        run: echo "GITHUB_REPO=${{ github.repository }}" >> $GITHUB_ENV

      - name: Get AWS Account ID
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
          terraform_wrapper: false

      - name: Verify Backend Infrastructure
        run: |
          echo "ğŸ” Verifying Terraform backend infrastructure..."
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket "thrive-cluster-test-terraform-state" 2>/dev/null; then
            echo "âœ… S3 backend bucket exists"
          else
            echo "âŒ S3 backend bucket not found!"
            echo "ğŸ’¡ Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi
          
          # Check if DynamoDB table exists
          if aws dynamodb describe-table --table-name "thrive-cluster-test-terraform-locks" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "âœ… DynamoDB lock table exists"
          else
            echo "âŒ DynamoDB lock table not found!"
            echo "ğŸ’¡ Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi

      - name: Terraform Init
        run: |
          echo "ğŸ”„ Initializing Terraform with S3 backend..."
          terraform init -backend-config="bucket=thrive-cluster-test-terraform-state" -backend-config="key=terraform.tfstate" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=thrive-cluster-test-terraform-locks" -backend-config="encrypt=true"
          echo "âœ… Terraform initialized with S3 backend"

      - name: Verify Terraform state
        run: |
          echo "ğŸ” Verifying Terraform state from S3 backend..."
          echo "ğŸ“‹ Resources in state:"
          terraform state list 2>/dev/null || echo "â„¹ï¸ No resources in state or state is empty"
          echo "âœ… State file is accessible from S3 backend"

      - name: Check if resources exist
        id: check-resources
        run: |
          if terraform state list 2>/dev/null | grep -q .; then
            echo "resources_exist=true" >> $GITHUB_OUTPUT
            echo "ğŸ“‹ Found existing resources to destroy"
            terraform state list
          else
            echo "resources_exist=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No resources found in Terraform state"
          fi

      - name: Check and handle Terraform state lock
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ” Checking for existing Terraform state locks..."
          
          # Try to run terraform plan to check for locks
          if ! terraform plan -var="github_repo=$GITHUB_REPO" -var="aws_account_id=$AWS_ACCOUNT_ID" -out=/dev/null 2>&1 | grep -q "Error acquiring the state lock"; then
            echo "âœ… No state lock detected"
          else
            echo "âš ï¸ State lock detected, attempting to force unlock..."
            
            # Extract lock ID from the error message
            LOCK_ID=$(terraform plan -var="github_repo=$GITHUB_REPO" -var="aws_account_id=$AWS_ACCOUNT_ID" -out=/dev/null 2>&1 | grep -o 'ID:\s*[a-f0-9-]*' | sed 's/ID:\s*//' | xargs || echo "")
            
            if [ -n "$LOCK_ID" ]; then
              echo "ğŸ”“ Force unlocking state with ID: '$LOCK_ID'"
              echo "ğŸ” Lock ID length: ${#LOCK_ID}"
              terraform force-unlock -force "$LOCK_ID" || echo "âš ï¸ Could not unlock state, will try destroy anyway"
            else
              echo "âš ï¸ Could not extract lock ID, will try destroy anyway"
            fi
          fi

      - name: Terraform Destroy
        if: steps.check-resources.outputs.resources_exist == 'true'
        timeout-minutes: 45
        run: |
          echo "ğŸ’¥ Destroying infrastructure with Terraform..."
          echo "â° This operation has a 45-minute timeout"
          echo "ğŸ“‹ Some resources (like VPC subnets) may take several minutes to destroy"
          
          # Try terraform destroy with lock handling
          if timeout 1800 terraform destroy \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_account_id=$AWS_ACCOUNT_ID" \
            -auto-approve; then
            echo "âœ… Infrastructure destroyed successfully!"
          else
            echo "âš ï¸ Terraform destroy failed, checking for state lock issues..."
            
            # Check if it's a lock issue and try to force unlock
            if terraform destroy \
              -var="github_repo=$GITHUB_REPO" \
              -var="aws_account_id=$AWS_ACCOUNT_ID" \
              -auto-approve 2>&1 | grep -q "Error acquiring the state lock"; then
              
              echo "ğŸ”“ State lock detected, attempting force unlock and retry..."
              
              # Extract lock ID and force unlock
              LOCK_ID=$(terraform destroy \
                -var="github_repo=$GITHUB_REPO" \
                -var="aws_account_id=$AWS_ACCOUNT_ID" \
                -auto-approve 2>&1 | grep -o 'ID:\s*[a-f0-9-]*' | sed 's/ID:\s*//' | xargs || echo "")
              
              if [ -n "$LOCK_ID" ]; then
                echo "ğŸ”“ Force unlocking state with ID: '$LOCK_ID'"
                echo "ğŸ” Lock ID length: ${#LOCK_ID}"
                terraform force-unlock -force "$LOCK_ID"
                
                echo "ğŸ”„ Retrying terraform destroy after unlock..."
                terraform destroy \
                  -var="github_repo=$GITHUB_REPO" \
                  -var="aws_account_id=$AWS_ACCOUNT_ID" \
                  -auto-approve
                echo "âœ… Infrastructure destroyed successfully after unlock!"
              else
                echo "âŒ Could not extract lock ID, destroy failed"
                exit 1
              fi
            else
              echo "âŒ Terraform destroy failed for reasons other than state lock"
              echo "ğŸ” Checking if it's a timeout issue..."
              
              # Check if it's a timeout (exit code 124)
              if [ $? -eq 124 ]; then
                echo "â° Terraform destroy timed out after 30 minutes"
                echo "ğŸ”„ Attempting to continue with manual cleanup of remaining resources..."
                echo "â„¹ï¸ Some resources may still exist and need manual cleanup"
              else
                echo "âŒ Terraform destroy failed with error code: $?"
                exit 1
              fi
            fi
          fi

      - name: Clean up remaining Kubernetes resources
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ§¹ Cleaning up any remaining Kubernetes resources..."
          # Check if EKS cluster still exists
          if aws eks describe-cluster --name thrive-cluster-test --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "ğŸ“¦ EKS cluster still exists, cleaning up Kubernetes resources..."
            
            # Update kubeconfig
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name thrive-cluster-test || echo "âš ï¸ Could not update kubeconfig"
            
            # Delete all deployments, services, and ingress
            kubectl delete deployment hello-world --ignore-not-found=true || echo "âš ï¸ Could not delete deployment"
            kubectl delete service hello-world-service --ignore-not-found=true || echo "âš ï¸ Could not delete service"
            kubectl delete service hello-world-clusterip --ignore-not-found=true || echo "âš ï¸ Could not delete clusterip service"
            kubectl delete ingress hello-world-ingress --ignore-not-found=true || echo "âš ï¸ Could not delete ingress"
            kubectl delete hpa hello-world-hpa --ignore-not-found=true || echo "âš ï¸ Could not delete HPA"
            kubectl delete networkpolicy hello-world-network-policy --ignore-not-found=true || echo "âš ï¸ Could not delete network policy"
            kubectl delete podsecuritypolicy hello-world-psp --ignore-not-found=true || echo "âš ï¸ Could not delete PSP"
            
            # Wait for resources to be deleted
            echo "â³ Waiting for Kubernetes resources to be deleted..."
            sleep 30
            
            # Force delete any remaining pods
            kubectl delete pods --all --force --grace-period=0 --ignore-not-found=true || echo "âš ï¸ Could not force delete pods"
            
            echo "âœ… Kubernetes resources cleaned up"
          else
            echo "â„¹ï¸ EKS cluster destroyed by Terraform"
          fi

      - name: Clean up remaining ECR images
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ§¹ Cleaning up any remaining ECR images..."
          # Check if ECR repository still exists
          if aws ecr describe-repositories --repository-names hello-world --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "ğŸ“¦ ECR repository still exists, cleaning up images..."
            # List and delete all images
            IMAGES=$(aws ecr list-images --repository-name hello-world --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')
            if [ "$IMAGES" != "[]" ] && [ "$IMAGES" != "null" ]; then
              echo "ğŸ—‘ï¸ Deleting ECR images..."
              aws ecr batch-delete-image --repository-name hello-world --region ${{ env.AWS_REGION }} --image-ids "$IMAGES" || echo "âš ï¸ Some images may have already been deleted"
            else
              echo "â„¹ï¸ No images found in ECR repository"
            fi
          else
            echo "â„¹ï¸ ECR repository destroyed by Terraform"
          fi

      - name: Clean up remaining CloudWatch Log Groups
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ§¹ Cleaning up any remaining CloudWatch Log Groups..."
          # Delete EKS cluster log group
          aws logs delete-log-group --log-group-name "/aws/eks/thrive-cluster-test/cluster" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ Log group not found or already deleted"
          echo "âœ… CloudWatch Log Groups cleaned up"

      - name: Clean up remaining SNS Topic Subscriptions
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ§¹ Cleaning up any remaining SNS Topic Subscriptions..."
          # List and delete SNS topic subscriptions
          TOPICS=$(aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null || echo "")
          if [ -n "$TOPICS" ]; then
            for topic in $TOPICS; do
              echo "ğŸ“§ Found SNS topic: $topic"
              # List subscriptions
              SUBSCRIPTIONS=$(aws sns list-subscriptions-by-topic --topic-arn "$topic" --region ${{ env.AWS_REGION }} --query 'Subscriptions[?SubscriptionArn != `PendingConfirmation`].SubscriptionArn' --output text 2>/dev/null || echo "")
              if [ -n "$SUBSCRIPTIONS" ]; then
                for sub in $SUBSCRIPTIONS; do
                  echo "ğŸ—‘ï¸ Deleting subscription: $sub"
                  aws sns unsubscribe --subscription-arn "$sub" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "âš ï¸ Could not delete subscription"
                done
              fi
            done
          fi
          echo "âœ… SNS Topic Subscriptions cleaned up"

      - name: Force cleanup of stubborn resources
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ”§ Force cleaning up any stubborn resources that Terraform couldn't destroy..."
          
          # Check for stubborn VPC resources
          echo "ğŸ” Checking for stubborn VPC resources..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=Thrive_Cluster_Test" --query 'Vpcs[0].VpcId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ]; then
            echo "âš ï¸ VPC still exists: $VPC_ID"
            echo "ğŸ”§ Attempting to force delete VPC resources..."
            
            # Delete any remaining ENIs
            aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query 'NetworkInterfaces[?Status==`available`].NetworkInterfaceId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-network-interface --network-interface-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ No ENIs to delete"
            
            # Delete any remaining security groups (except default)
            aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-security-group --group-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ No security groups to delete"
            
            # Delete any remaining subnets
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[].SubnetId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-subnet --subnet-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ No subnets to delete"
            
            # Delete any remaining route tables (except main)
            aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text --region ${{ env.AWS_REGION }} | xargs -r aws ec2 delete-route-table --route-table-id --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ No route tables to delete"
            
            # Delete internet gateway
            IGW_ID=$(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[0].InternetGatewayId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
            if [ "$IGW_ID" != "None" ] && [ "$IGW_ID" != "null" ]; then
              aws ec2 detach-internet-gateway --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ Could not detach IGW"
              aws ec2 delete-internet-gateway --internet-gateway-id "$IGW_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ Could not delete IGW"
            fi
            
            # Finally delete the VPC
            aws ec2 delete-vpc --vpc-id "$VPC_ID" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ Could not delete VPC"
            echo "âœ… VPC cleanup attempted"
          else
            echo "â„¹ï¸ No VPC found or already deleted"
          fi
          
          # Force delete any remaining ECR images
          aws ecr batch-delete-image --repository-name hello-world --region ${{ env.AWS_REGION }} --image-ids "$(aws ecr list-images --repository-name hello-world --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')" 2>/dev/null || echo "â„¹ï¸ No ECR images to force delete"
          
          # Force delete any remaining CloudWatch log groups
          aws logs delete-log-group --log-group-name "/aws/eks/thrive-cluster-test/cluster" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ No log groups to force delete"
          
          # Force delete any remaining SNS subscriptions
          for topic in $(aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null || echo ""); do
            for sub in $(aws sns list-subscriptions-by-topic --topic-arn "$topic" --region ${{ env.AWS_REGION }} --query 'Subscriptions[?SubscriptionArn != `PendingConfirmation`].SubscriptionArn' --output text 2>/dev/null || echo ""); do
              aws sns unsubscribe --subscription-arn "$sub" --region ${{ env.AWS_REGION }} 2>/dev/null || echo "â„¹ï¸ Could not force delete subscription"
            done
          done
          
          echo "âœ… Force cleanup completed"

      - name: Verify destruction
        if: steps.check-resources.outputs.resources_exist == 'true'
        run: |
          echo "ğŸ” Verifying destruction of all resources..."
          
          # Check EKS cluster
          if aws eks describe-cluster --name thrive-cluster-test --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ EKS cluster still exists"
          else
            echo "âœ… EKS cluster destroyed"
          fi
          
          # Check ECR repository
          if aws ecr describe-repositories --repository-names hello-world --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ ECR repository still exists"
          else
            echo "âœ… ECR repository destroyed"
          fi
          
          # Check VPC
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=Thrive_Cluster_Test" --query 'Vpcs[0].VpcId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
          if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ]; then
            echo "âš ï¸ VPC still exists: $VPC_ID"
          else
            echo "âœ… VPC destroyed"
          fi
          
          # Check IAM Roles
          if aws iam get-role --role-name thrive-cluster-test-cluster-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ EKS cluster IAM role still exists"
          else
            echo "âœ… EKS cluster IAM role destroyed"
          fi
          
          if aws iam get-role --role-name thrive-cluster-test-node-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ EKS node IAM role still exists"
          else
            echo "âœ… EKS node IAM role destroyed"
          fi
          
          if aws iam get-role --role-name thrive-cluster-test-github-actions-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ GitHub Actions IAM role still exists"
          else
            echo "âœ… GitHub Actions IAM role destroyed"
          fi
          
          if aws iam get-role --role-name hello-world-pod-role --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ Hello World pod IAM role still exists"
          else
            echo "âœ… Hello World pod IAM role destroyed"
          fi
          
          # Check Secrets Manager
          if aws secretsmanager describe-secret --secret-id thrive-cluster-test-github-actions-credentials --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ GitHub Actions secret still exists"
          else
            echo "âœ… GitHub Actions secret destroyed"
          fi
          
          if aws secretsmanager describe-secret --secret-id thrive-cluster-test-app-secrets --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ App secrets still exist"
          else
            echo "âœ… App secrets destroyed"
          fi
          
          # Check CloudWatch Log Groups
          if aws logs describe-log-groups --log-group-name-prefix "/aws/eks/thrive-cluster-test" --region ${{ env.AWS_REGION }} --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ CloudWatch log groups still exist"
          else
            echo "âœ… CloudWatch log groups destroyed"
          fi
          
          # Check CloudWatch Dashboards
          if aws cloudwatch list-dashboards --region ${{ env.AWS_REGION }} --query 'DashboardEntries[?contains(DashboardName, `thrive-cluster-test`)].DashboardName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ CloudWatch dashboards still exist"
          else
            echo "âœ… CloudWatch dashboards destroyed"
          fi
          
          # Check SNS Topics
          if aws sns list-topics --region ${{ env.AWS_REGION }} --query 'Topics[?contains(TopicArn, `thrive-cluster-test`)].TopicArn' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ SNS topics still exist"
          else
            echo "âœ… SNS topics destroyed"
          fi
          
          # Check CloudWatch Alarms
          if aws cloudwatch describe-alarms --alarm-names thrive-cluster-test-billing-alarm --region ${{ env.AWS_REGION }} --query 'MetricAlarms[0].AlarmName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ CloudWatch alarms still exist"
          else
            echo "âœ… CloudWatch alarms destroyed"
          fi
          
          # Check Budgets
          if aws budgets describe-budgets --account-id $(aws sts get-caller-identity --query Account --output text) --region ${{ env.AWS_REGION }} --query 'Budgets[?contains(BudgetName, `thrive-cluster-test`)].BudgetName' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ Budgets still exist"
          else
            echo "âœ… Budgets destroyed"
          fi
          
          # Check OIDC Providers
          if aws iam list-open-id-connect-providers --region ${{ env.AWS_REGION }} --query 'OpenIDConnectProviderList[?contains(Arn, `thrive-cluster-test`)].Arn' --output text 2>/dev/null | grep -q "thrive-cluster-test"; then
            echo "âš ï¸ OIDC providers still exist"
          else
            echo "âœ… OIDC providers destroyed"
          fi

      - name: Destruction Summary
        run: |
          echo "ğŸ¯ Infrastructure Destruction Complete!"
          echo ""
          echo "ğŸ“‹ Resources that were cleaned up:"
          echo "  âœ… EKS Cluster and Node Groups"
          echo "  âœ… VPC, Subnets, Internet Gateway, Route Tables"
          echo "  âœ… ECR Repository and Images"
          echo "  âœ… IAM Roles and Policies (EKS, GitHub Actions, Pod)"
          echo "  âœ… Secrets Manager Secrets"
          echo "  âœ… CloudWatch Log Groups, Dashboards, and Alarms"
          echo "  âœ… SNS Topics and Subscriptions"
          echo "  âœ… AWS Budgets and Billing Alarms"
          echo "  âœ… OIDC Identity Providers"
          echo "  âœ… Kubernetes Resources (Deployments, Services, Ingress, etc.)"
          echo ""
          echo "ğŸ’° All AWS resources have been destroyed"
          echo "ğŸ’¡ You can now run the deploy workflow to recreate everything"
          echo "ğŸ“Š Check your AWS Console to verify all resources are gone"
          echo "ğŸ”’ Your AWS account is now clean and ready for the next deployment"
