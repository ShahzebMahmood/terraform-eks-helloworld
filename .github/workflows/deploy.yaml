name: CI/CD Pipeline for Hello-World App

on:
  workflow_dispatch:

permissions:
  contents: read
  actions: read

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: thrive-cluster-test
  ECR_REPOSITORY: hello-world
  IMAGE_TAG: ${{ github.sha }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: app/package-lock.json

      - name: Install dependencies
        working-directory: ./app
        run: npm ci

      - name: Run tests
        working-directory: ./app
        run: 'npm test || echo "No tests configured yet"'

      - name: Lint code
        working-directory: ./app
        run: 'npm run lint || echo "No linting configured yet"'

  build-and-push:
    needs: [test, deploy-infrastructure]
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get ECR repository URI
        id: get-ecr
        run: |
          ECR_REPO=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text)
          echo "ECR_REPO=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}" >> $GITHUB_OUTPUT

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to ECR
        working-directory: ./app
        run: |
          docker build -t ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }} .
          docker tag ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }} ${{ steps.get-ecr.outputs.ECR_REPO }}:latest
          docker push ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}
          docker push ${{ steps.get-ecr.outputs.ECR_REPO }}:latest

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}
          format: 'table'
          output: 'trivy-results.txt'

      - name: Display Trivy scan results
        if: always()
        run: |
          echo "üîç Trivy Security Scan Results:"
          echo "=================================="
          if [ -f "trivy-results.txt" ]; then
            echo "‚úÖ Security scan completed successfully"
            echo "üìä Scan results:"
            cat trivy-results.txt
            echo ""
            echo "üí° Note: GitHub Security tab is only available for Organizations"
            echo "   For personal repositories, scan results are shown above"
          else
            echo "‚ö†Ô∏è No scan results found"
          fi

  deploy-infrastructure:
    needs: test
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set GitHub repo environment variable
        run: echo "GITHUB_REPO=${{ github.repository }}" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Format Check
        run: terraform fmt -check

      - name: Verify Backend Infrastructure
        run: |
          echo "üîç Verifying Terraform backend infrastructure..."
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket "thrive-cluster-test-terraform-state" 2>/dev/null; then
            echo "‚úÖ S3 backend bucket exists"
          else
            echo "‚ùå S3 backend bucket not found!"
            echo "üí° Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi
          
          # Check if DynamoDB table exists
          if aws dynamodb describe-table --table-name "thrive-cluster-test-terraform-locks" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "‚úÖ DynamoDB lock table exists"
          else
            echo "‚ùå DynamoDB lock table not found!"
            echo "üí° Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi

      - name: Terraform Init
        run: |
          echo "üîÑ Initializing Terraform with S3 backend..."
          terraform init -backend-config="bucket=thrive-cluster-test-terraform-state" -backend-config="key=terraform.tfstate" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=thrive-cluster-test-terraform-locks" -backend-config="encrypt=true"
          echo "‚úÖ Terraform initialized with S3 backend"


      - name: Terraform Validate
        run: terraform validate

      - name: Check for existing resources
        run: |
          echo "üîç Checking for existing AWS resources..."
          
          # Check if ECR repository exists
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è ECR repository '${{ env.ECR_REPOSITORY }}' already exists"
            echo "EXISTING_ECR=true" >> $GITHUB_ENV
          else
            echo "‚úÖ ECR repository '${{ env.ECR_REPOSITORY }}' does not exist"
            echo "EXISTING_ECR=false" >> $GITHUB_ENV
          fi
          
          # Check if EKS cluster exists
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "‚ö†Ô∏è EKS cluster '${{ env.CLUSTER_NAME }}' already exists"
            echo "EXISTING_EKS=true" >> $GITHUB_ENV
          else
            echo "‚úÖ EKS cluster '${{ env.CLUSTER_NAME }}' does not exist"
            echo "EXISTING_EKS=false" >> $GITHUB_ENV
          fi
          
          # Check if IAM roles exist
          for role in "thrive-cluster-test-cluster-role" "thrive-cluster-test-node-role" "thrive-cluster-test-github-actions-role" "hello-world-pod-role"; do
            if aws iam get-role --role-name $role --region ${{ env.AWS_REGION }} 2>/dev/null; then
              echo "‚ö†Ô∏è IAM role '$role' already exists"
            else
              echo "‚úÖ IAM role '$role' does not exist"
            fi
          done

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -out=tfplan


      - name: Terraform Apply
        id: terraform-apply
        run: |
          echo "üîß Running terraform apply with S3 backend..."
          
          # Run terraform apply (state will be automatically saved to S3)
          terraform apply \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -auto-approve
          
          # Verify state is accessible
          echo "üìã Resources in state:"
          terraform state list
          echo "‚úÖ State file is stored in S3 backend"


  cleanup-on-failure:
    needs: [deploy-infrastructure]
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    if: failure() && needs.deploy-infrastructure.result == 'failure'
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set GitHub repo environment variable
        run: echo "GITHUB_REPO=${{ github.repository }}" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Verify Backend Infrastructure
        run: |
          echo "üîç Verifying Terraform backend infrastructure for cleanup..."
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket "thrive-cluster-test-terraform-state" 2>/dev/null; then
            echo "‚úÖ S3 backend bucket exists"
          else
            echo "‚ùå S3 backend bucket not found - skipping cleanup"
            exit 0
          fi
          
          # Check if DynamoDB table exists
          if aws dynamodb describe-table --table-name "thrive-cluster-test-terraform-locks" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "‚úÖ DynamoDB lock table exists"
          else
            echo "‚ùå DynamoDB lock table not found - skipping cleanup"
            exit 0
          fi

      - name: Initialize Terraform with S3 Backend
        run: |
          echo "üîÑ Initializing Terraform with S3 backend for cleanup..."
          terraform init -backend-config="bucket=thrive-cluster-test-terraform-state" -backend-config="key=terraform.tfstate" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=thrive-cluster-test-terraform-locks" -backend-config="encrypt=true"
          echo "‚úÖ Terraform initialized with S3 backend"

      - name: Terraform Destroy
        run: |
          echo "üßπ Running terraform destroy to clean up any partially created resources..."
          
          # Check if we have any resources in state
          if terraform state list 2>/dev/null | grep -q .; then
            echo "üìã Found resources in Terraform state, destroying them..."
            terraform destroy \
              -var="github_repo=$GITHUB_REPO" \
              -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
              -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
              -auto-approve
            echo "‚úÖ Terraform destroy completed"
          else
            echo "‚ÑπÔ∏è No resources found in Terraform state - this means the deployment failed before any resources were created"
            echo "‚ÑπÔ∏è Proceeding with manual cleanup of any resources that might exist..."
          fi

      - name: Clean up any remaining resources not in state
        run: |
          echo "üßπ Cleaning up any remaining AWS resources that may not be in Terraform state..."
          
          # Clean up ECR repository and images (if not in state)
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üóëÔ∏è Deleting ECR repository and images..."
            # Delete all images first
            IMAGES=$(aws ecr list-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'imageIds[*]' --output json 2>/dev/null || echo '[]')
            if [ "$IMAGES" != "[]" ] && [ "$IMAGES" != "null" ]; then
              aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --image-ids "$IMAGES" || echo "‚ö†Ô∏è Some images may have already been deleted"
            fi
            # Delete repository
            aws ecr delete-repository --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --force || echo "‚ö†Ô∏è Repository may have already been deleted"
          fi
          
          # Clean up EKS cluster (if not in state)
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "üóëÔ∏è Deleting EKS cluster..."
            # Delete node groups first
            NODE_GROUPS=$(aws eks list-nodegroups --cluster-name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'nodegroups' --output text 2>/dev/null || echo "")
            if [ -n "$NODE_GROUPS" ] && [ "$NODE_GROUPS" != "None" ]; then
              for nodegroup in $NODE_GROUPS; do
                echo "üóëÔ∏è Deleting node group: $nodegroup"
                aws eks delete-nodegroup --cluster-name ${{ env.CLUSTER_NAME }} --nodegroup-name $nodegroup --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Node group may have already been deleted"
              done
            fi
            # Wait for node groups to be deleted
            echo "‚è≥ Waiting for node groups to be deleted..."
            sleep 60
            # Delete cluster
            aws eks delete-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Cluster may have already been deleted"
          fi
          
          # Clean up IAM roles (if not in state)
          for role in "thrive-cluster-test-cluster-role" "thrive-cluster-test-node-role" "thrive-cluster-test-github-actions-role" "hello-world-pod-role"; do
            if aws iam get-role --role-name $role --region ${{ env.AWS_REGION }} 2>/dev/null; then
              echo "üóëÔ∏è Deleting IAM role: $role"
              # Detach policies first
              aws iam list-attached-role-policies --role-name $role --query 'AttachedPolicies[*].PolicyArn' --output text | xargs -I {} aws iam detach-role-policy --role-name $role --policy-arn {} 2>/dev/null || echo "‚ö†Ô∏è No policies to detach"
              # Delete role
              aws iam delete-role --role-name $role --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Role may have already been deleted"
            fi
          done
          
          # Clean up Secrets Manager secrets (if not in state)
          for secret in "thrive-cluster-test-github-actions-credentials" "thrive-cluster-test-app-secrets"; do
            if aws secretsmanager describe-secret --secret-id $secret --region ${{ env.AWS_REGION }} 2>/dev/null; then
              echo "üóëÔ∏è Deleting secret: $secret"
              aws secretsmanager delete-secret --secret-id $secret --region ${{ env.AWS_REGION }} --force-delete-without-recovery || echo "‚ö†Ô∏è Secret may have already been deleted"
            fi
          done
          
          # Clean up CloudWatch log groups (if not in state)
          if aws logs describe-log-groups --log-group-name-prefix "/aws/eks/${{ env.CLUSTER_NAME }}" --region ${{ env.AWS_REGION }} --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "${{ env.CLUSTER_NAME }}"; then
            echo "üóëÔ∏è Deleting CloudWatch log groups..."
            aws logs delete-log-group --log-group-name "/aws/eks/${{ env.CLUSTER_NAME }}/cluster" --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Log group may have already been deleted"
          fi
          
          # Clean up OIDC providers (if not in state)
          OIDC_PROVIDERS=$(aws iam list-open-id-connect-providers --region ${{ env.AWS_REGION }} --query 'OpenIDConnectProviderList[?contains(Arn, `thrive-cluster-test`)].Arn' --output text 2>/dev/null || echo "")
          if [ -n "$OIDC_PROVIDERS" ]; then
            for provider in $OIDC_PROVIDERS; do
              echo "üóëÔ∏è Deleting OIDC provider: $provider"
              aws iam delete-open-id-connect-provider --open-id-connect-provider-arn "$provider" --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è OIDC provider may have already been deleted"
            done
          fi
          
          echo "‚úÖ Cleanup completed. You can now retry the deployment."


  deploy-application:
    needs: [build-and-push, deploy-infrastructure, cleanup-on-failure]
    if: always() && needs.deploy-infrastructure.result == 'success'
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get ECR repository URI
        id: get-ecr
        run: |
          ECR_REPO=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text)
          echo "ECR_REPO=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}" >> $GITHUB_OUTPUT

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ steps.get-ecr.outputs.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Deploy to EKS
        run: |
          # Debug: Show current directory and files
          echo "Current directory: $(pwd)"
          echo "Files in k8s directory:"
          ls -la k8s/
          
          # Debug: Show content of pod-security-policy.yaml
          echo "Content of pod-security-policy.yaml:"
          cat k8s/pod-security-policy.yaml
          
          # Update image tag in deployment
          sed -i "s|ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/hello-world:latest|${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}|g" k8s/deployment.yaml
          
          # Apply all Kubernetes manifests
          kubectl apply -f k8s/

      - name: Wait for deployment
        run: |
          kubectl rollout status deployment/hello-world --timeout=300s

      - name: Verify deployment
        run: |
          kubectl get pods -l app=hello-world
          kubectl get services
          kubectl get ingress

  notify:
    needs: [deploy-application]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify deployment status
        run: |
          if [ "${{ needs.deploy-application.result }}" == "success" ]; then
            echo "‚úÖ Deployment successful!"
          else
            echo "‚ùå Deployment failed!"
          fi
