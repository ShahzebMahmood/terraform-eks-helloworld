name: CI/CD Pipeline for Hello-World App

on:
  workflow_dispatch:

permissions:
  contents: read
  actions: read

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: thrive-cluster-test
  ECR_REPOSITORY: hello-world
  IMAGE_TAG: ${{ github.sha }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: app/package-lock.json

      - name: Install dependencies
        working-directory: ./app
        run: npm ci

      - name: Run tests
        working-directory: ./app
        run: 'npm test || echo "No tests configured yet"'

      - name: Lint code
        working-directory: ./app
        run: 'npm run lint || echo "No linting configured yet"'

  build-and-push:
    needs: [test, deploy-infrastructure]
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get ECR repository URI
        id: get-ecr
        run: |
          ECR_REPO=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text)
          echo "ECR_REPO=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}" >> $GITHUB_OUTPUT

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to ECR
        working-directory: ./app
        run: |
          docker build -t ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }} .
          docker tag ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }} ${{ steps.get-ecr.outputs.ECR_REPO }}:latest
          docker push ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}
          docker push ${{ steps.get-ecr.outputs.ECR_REPO }}:latest

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}
          format: 'table'
          output: 'trivy-results.txt'

      - name: Display Trivy scan results
        if: always()
        run: |
          echo "ğŸ” Trivy Security Scan Results:"
          echo "=================================="
          if [ -f "trivy-results.txt" ]; then
            echo "âœ… Security scan completed successfully"
            echo "ğŸ“Š Scan results:"
            cat trivy-results.txt
            echo ""
            echo "ğŸ’¡ Note: GitHub Security tab is only available for Organizations"
            echo "   For personal repositories, scan results are shown above"
          else
            echo "âš ï¸ No scan results found"
          fi

  deploy-infrastructure:
    needs: test
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set GitHub repo environment variable
        run: echo "GITHUB_REPO=${{ github.repository }}" >> $GITHUB_ENV

      - name: Get AWS Account ID
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Format Check
        run: terraform fmt -check

      - name: Verify Backend Infrastructure
        run: |
          echo "ğŸ” Verifying Terraform backend infrastructure..."
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket "thrive-cluster-test-terraform-state" 2>/dev/null; then
            echo "âœ… S3 backend bucket exists"
          else
            echo "âŒ S3 backend bucket not found!"
            echo "ğŸ’¡ Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi
          
          # Check if DynamoDB table exists
          if aws dynamodb describe-table --table-name "thrive-cluster-test-terraform-locks" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "âœ… DynamoDB lock table exists"
          else
            echo "âŒ DynamoDB lock table not found!"
            echo "ğŸ’¡ Please run the 'Setup Terraform Backend' workflow first"
            exit 1
          fi

      - name: Terraform Init
        run: |
          echo "ğŸ”„ Initializing Terraform with S3 backend..."
          terraform init -backend-config="bucket=thrive-cluster-test-terraform-state" -backend-config="key=terraform.tfstate" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=thrive-cluster-test-terraform-locks" -backend-config="encrypt=true"
          echo "âœ… Terraform initialized with S3 backend"


      - name: Terraform Validate
        run: terraform validate

      - name: Check for existing resources
        run: |
          echo "ğŸ” Checking for existing AWS resources..."
          
          # Check if ECR repository exists
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ ECR repository '${{ env.ECR_REPOSITORY }}' already exists"
            echo "EXISTING_ECR=true" >> $GITHUB_ENV
          else
            echo "âœ… ECR repository '${{ env.ECR_REPOSITORY }}' does not exist"
            echo "EXISTING_ECR=false" >> $GITHUB_ENV
          fi
          
          # Check if EKS cluster exists
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "âš ï¸ EKS cluster '${{ env.CLUSTER_NAME }}' already exists"
            echo "EXISTING_EKS=true" >> $GITHUB_ENV
          else
            echo "âœ… EKS cluster '${{ env.CLUSTER_NAME }}' does not exist"
            echo "EXISTING_EKS=false" >> $GITHUB_ENV
          fi
          
          # Check if IAM roles exist
          for role in "thrive-cluster-test-cluster-role" "thrive-cluster-test-node-role" "thrive-cluster-test-github-actions-role" "hello-world-pod-role"; do
            if aws iam get-role --role-name $role --region ${{ env.AWS_REGION }} 2>/dev/null; then
              echo "âš ï¸ IAM role '$role' already exists"
            else
              echo "âœ… IAM role '$role' does not exist"
            fi
          done

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_account_id=$AWS_ACCOUNT_ID" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -out=tfplan


      - name: Terraform Apply
        id: terraform-apply
        run: |
          echo "ğŸ”§ Running terraform apply with S3 backend..."
          
          # Run terraform apply (state will be automatically saved to S3)
          terraform apply \
            -var="github_repo=$GITHUB_REPO" \
            -var="aws_account_id=$AWS_ACCOUNT_ID" \
            -var="aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -auto-approve
          
          # Verify state is accessible
          echo "ğŸ“‹ Resources in state:"
          terraform state list
          echo "âœ… State file is stored in S3 backend"


  deploy-application:
    needs: [build-and-push, deploy-infrastructure]
    if: always() && needs.deploy-infrastructure.result == 'success'
    runs-on: ubuntu-latest
    environment: AWS_ACCESS_KEY_ID
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get ECR repository URI and AWS Account ID
        id: get-ecr
        run: |
          ECR_REPO=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text)
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ECR_REPO=$ECR_REPO" >> $GITHUB_OUTPUT
          echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}" >> $GITHUB_OUTPUT
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ steps.get-ecr.outputs.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Deploy to EKS
        run: |
          # Debug: Show current directory and files
          echo "Current directory: $(pwd)"
          echo "Files in k8s directory:"
          ls -la k8s/
          
          # Update placeholders in deployment.yaml
          echo "ğŸ”§ Updating placeholders in deployment.yaml..."
          
          # First, replace the IAM role ARN placeholder (more specific pattern)
          sed -i "s|arn:aws:iam::ACCOUNT_ID:role/hello-world-pod-role|arn:aws:iam::${{ steps.get-ecr.outputs.AWS_ACCOUNT_ID }}:role/hello-world-pod-role|g" k8s/deployment.yaml
          
          # Then, replace the ECR image URI placeholder
          sed -i "s|ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/hello-world:latest|${{ steps.get-ecr.outputs.ECR_REPO }}:${{ env.IMAGE_TAG }}|g" k8s/deployment.yaml
          
          # Show the updated deployment file for debugging
          echo "ğŸ“‹ Updated deployment.yaml:"
          cat k8s/deployment.yaml
          
          # Step 1: Create namespace first
          echo "ğŸ—ï¸ Creating namespace and PodDisruptionBudget..."
          kubectl apply -f k8s/pod-security-policy.yaml
          
          # Step 2: Wait for namespace to be ready (using a simple check)
          echo "â³ Waiting for namespace to be ready..."
          until kubectl get namespace hello-world >/dev/null 2>&1; do
            echo "Waiting for namespace to be created..."
            sleep 2
          done
          echo "âœ… Namespace is ready"
          
          # Step 3: Apply all other Kubernetes manifests with error checking
          echo "ğŸš€ Deploying application resources..."
          
          echo "ğŸ“¦ Applying deployment..."
          if kubectl apply -f k8s/deployment.yaml; then
            echo "âœ… Deployment applied successfully"
          else
            echo "âŒ Deployment failed - showing detailed error:"
            kubectl apply -f k8s/deployment.yaml --dry-run=client -o yaml
            exit 1
          fi
          
          echo "ğŸ”— Applying services..."
          kubectl apply -f k8s/service.yaml || { echo "âŒ Service failed"; exit 1; }
          kubectl apply -f k8s/service-clusterip.yaml || { echo "âŒ ClusterIP service failed"; exit 1; }
          
          echo "ğŸŒ Applying ingress..."
          kubectl apply -f k8s/ingress.yaml || { echo "âŒ Ingress failed"; exit 1; }
          
          echo "ğŸ“ˆ Applying HPA..."
          kubectl apply -f k8s/hpa.yaml || { echo "âŒ HPA failed"; exit 1; }
          
          echo "ğŸ”’ Applying network policy..."
          kubectl apply -f k8s/network-policy.yaml || { echo "âŒ Network policy failed"; exit 1; }
          
          echo "âœ… All Kubernetes resources applied successfully!"

      - name: Verify deployment exists
        run: |
          echo "ğŸ” Verifying deployment was created..."
          kubectl get deployment hello-world -n hello-world || { echo "âŒ Deployment not found"; exit 1; }
          echo "âœ… Deployment found"

      - name: Wait for deployment
        run: |
          echo "â³ Waiting for deployment to be ready..."
          kubectl rollout status deployment/hello-world -n hello-world --timeout=300s

      - name: Verify deployment
        run: |
          echo "ğŸ” Final verification..."
          kubectl get pods -l app=hello-world -n hello-world
          kubectl get services -n hello-world
          kubectl get ingress -n hello-world

  notify:
    needs: [deploy-application]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify deployment status
        run: |
          if [ "${{ needs.deploy-application.result }}" == "success" ]; then
            echo "âœ… Deployment successful!"
          else
            echo "âŒ Deployment failed!"
          fi